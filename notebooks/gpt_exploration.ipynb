{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skm-M8L0syfJ"
      },
      "outputs": [],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "YyF53ke1s7eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(\"Do not meddle in the affairs of wizards\")"
      ],
      "metadata": {
        "id": "VP0UD4LltABn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write python code to print the textual tokens in sequential order from a string, using the above tokenizer\n",
        "\n",
        "print(\"Encoded text:\", tokenizer.convert_ids_to_tokens(encoded))\n"
      ],
      "metadata": {
        "id": "x3XsqkMQtO6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded)"
      ],
      "metadata": {
        "id": "NdmylpO0tTWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input.keys())\n",
        "print(encoded_input['input_ids'])"
      ],
      "metadata": {
        "id": "25G-Qm1wta7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence-transformers"
      ],
      "metadata": {
        "id": "ps2vCs-3yQmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "nqqPuLfDz4Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"quick\", \"fast\", \"red\", \"blue\", \"ferari\"]\n",
        "single_word_embeddings = model.encode(words)\n",
        "\n",
        "for word, embed in zip(words, single_word_embeddings):\n",
        "  print(\"word: \", word)\n",
        "  print(\"embed: \", embed[0:10])\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "pgvUUF_q0Mmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: python code to compute the matrix of cosines between all of the pairs of words in the list above.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cos_sim = cosine_similarity(single_word_embeddings)\n",
        "print(cos_sim)\n"
      ],
      "metadata": {
        "id": "96EPNlK65mWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "for sentence in sentences:\n",
        "    print(\"Sentence:\", sentence)\n",
        "print(\"\")\n",
        "print(\"Cosine similarity between the first two sentences:\", cosine(embeddings[0], embeddings[1]))\n",
        "print(\"Cosine similarity between the second and third sentences:\", cosine(embeddings[1], embeddings[2]))\n",
        "print(\"Cosine similarity between the first and third sentences:\", cosine(embeddings[0], embeddings[2]))"
      ],
      "metadata": {
        "id": "AsVV5lSf8hoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load the pretrained tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Disable gradient calculation for inference\n",
        "with torch.no_grad():\n",
        "    # Sentence to encode\n",
        "    input_text = \"A fly can fly\"\n",
        "\n",
        "    # Tokenize the input text and convert to tensor\n",
        "    encoded_input = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "    # Get the model's output (hidden states)\n",
        "    outputs = model(**encoded_input)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    # Get all token ids to skip special tokens if desired\n",
        "    token_ids = encoded_input.input_ids[0].tolist()\n",
        "\n",
        "    # Print the embeddings for each token in the sentence\n",
        "    for i, token_id in enumerate(token_ids):\n",
        "      token = tokenizer.decode([token_id])\n",
        "      embedding = last_hidden_states[0, i]\n",
        "      print(f\"Token: {token} - Embedding: {embedding}\")"
      ],
      "metadata": {
        "id": "nlkYUW1IV2qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the sentences\n",
        "sentence1 = \"Time flies like an arrow\"\n",
        "sentence2 = \"Fruit flies like a banana\"\n",
        "\n",
        "# Tokenize and encode the sentences\n",
        "encoded_sentence1 = tokenizer(sentence1, return_tensors='pt')\n",
        "encoded_sentence2 = tokenizer(sentence2, return_tensors='pt')\n",
        "\n",
        "# Identify the 'like' token index\n",
        "word = \"like\"\n",
        "word_index1 = tokenizer.encode(sentence1, add_special_tokens=True).index(tokenizer.encode(word)[1]) - 1\n",
        "word_index2 = tokenizer.encode(sentence2, add_special_tokens=True).index(tokenizer.encode(word)[1]) - 1\n",
        "\n",
        "# Get the embeddings for each encoded sentence\n",
        "with torch.no_grad(): # Disable gradient tracking\n",
        "    output_sentence1 = model(**encoded_sentence1)\n",
        "    output_sentence2 = model(**encoded_sentence2)\n",
        "\n",
        "# Extract the embeddings for the word 'like' from both sentences using their indices\n",
        "embedding_sentence1 = output_sentence1.last_hidden_state[0, word_index1]\n",
        "embedding_sentence2 = output_sentence2.last_hidden_state[0, word_index2]\n",
        "\n",
        "# Calculate the cosine similarity\n",
        "similarity = 1 - cosine(embedding_sentence1.detach().numpy(), embedding_sentence2.detach().numpy())\n",
        "\n",
        "# Print the results\n",
        "print(\"Cosine Similarity:\", similarity)"
      ],
      "metadata": {
        "id": "uTK9rsg-QIXc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}